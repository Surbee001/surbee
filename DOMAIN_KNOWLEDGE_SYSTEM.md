# Surbee Domain Knowledge System
## Comprehensive Survey Methodology Across All Professions

This document serves as the foundational knowledge base for Surbee's AI to understand survey design across every professional domain, incorporating PhD-level research methodology and academic best practices.

---

## Table of Contents
1. [Universal Survey Methodology Principles](#universal-principles)
2. [Domain-Specific Survey Types](#domain-specific)
3. [Validated Instruments & Scales](#validated-instruments)
4. [Question Design Principles](#question-design)
5. [Cognitive Load & User Experience](#cognitive-load)
6. [Academic Best Practices](#academic-best-practices)

---

## Universal Survey Methodology Principles

### Core Methodological Framework (CROSS Guidelines 2024)

Based on the CROSS (Consensus-based Reporting of Survey Studies) checklist with 40 items:

#### 1. Research Question Development
- **Define population**: Clearly specify target demographic
- **Specify objectives**: What specific information needs to be gathered
- **Establish construct validity**: Ensure questions measure what they intend to measure
- **Framework-based validity argument**: Integrate throughout the survey design

#### 2. Sampling Approaches
**Probability Sampling:**
- Simple random sampling
- Stratified sampling
- Cluster sampling
- Systematic sampling

**Nonprobability Sampling:**
- Convenience sampling
- Purposive sampling
- Snowball sampling
- Quota sampling

**Modern Hybrid Approaches:**
- Probability samples recruited using residential addresses
- Opt-in online panels with statistical weighting
- Address-based sampling (ABS) for online surveys

#### 3. Validity Evidence (Standards for Educational & Psychological Testing)
- **Content validity**: Does the survey cover the full domain?
- **Response process validity**: Do respondents interpret questions as intended?
- **Internal structure**: Do items correlate appropriately?
- **Relations with other variables**: Convergent and discriminant validity
- **Consequences**: What are the implications of survey results?

#### 4. Reliability Testing
- **Cronbach's alpha**: Internal consistency (α ≥ 0.70 for exploratory, ≥ 0.80 for confirmatory)
- **Test-retest reliability**: Stability over time
- **Inter-rater reliability**: Consistency across raters
- **Parallel forms reliability**: Equivalence of alternate versions

---

## Domain-Specific Survey Types

### Healthcare & Medical

#### Common Survey Types:
1. **Patient Satisfaction (CAHPS Family)**
   - Access to care domain
   - Communication with providers
   - Care coordination
   - Responsiveness of staff

2. **Clinical Assessments**
   - Validated instruments: PHQ-9 (depression), GAD-7 (anxiety)
   - Quality of Life measures: SF-36, EQ-5D
   - Disease-specific questionnaires

3. **Healthcare Professional Surveys**
   - Burnout: Copenhagen Burnout Inventory
   - Well-being: 9-item Well-Being Index (WBI)
   - Job satisfaction scales

4. **Public Health**
   - Behavioral Risk Factor Surveillance System (BRFSS) style
   - Health behavior assessments
   - Epidemiological surveys

**Best Practices:**
- Use validated clinical instruments when measuring health outcomes
- Follow HIPAA-compliant language patterns
- Employ clinically appropriate terminology
- Include informed consent for sensitive health information
- Use Likert scales appropriate for health contexts (often 5-point)

**Key Databases:**
- CINAHL: Nursing and allied health literature
- PubMed: Medical research instruments
- HaPI: Health and Psychosocial Instruments

---

### Education & Academia

#### Common Survey Types:
1. **Student Engagement & Satisfaction**
   - Course evaluation questionnaires
   - Program assessment surveys
   - Student experience questionnaires

2. **Learning Outcomes Assessment**
   - Bloom's Taxonomy-based assessment
   - Competency-based evaluation
   - Skills acquisition surveys

3. **Faculty & Staff Surveys**
   - Teaching effectiveness
   - Professional development needs
   - Institutional climate

4. **Educational Research**
   - Pre-post intervention studies
   - Quasi-experimental designs
   - Longitudinal tracking

**Best Practices:**
- Align questions with learning objectives
- Use pedagogically sound question structures
- Apply appropriate assessment levels (Bloom's Taxonomy)
- Ensure accessibility for diverse learners
- Include both formative and summative elements

**Standards:**
- IPEDS (Integrated Postsecondary Education Data System) for institutional data
- AMEE Guide No. 87 for educational questionnaires

---

### Finance & Banking

#### Common Survey Types:
1. **Customer Satisfaction**
   - Net Promoter Score (NPS) - standard in finance
   - Service quality assessments
   - Digital banking experience

2. **Financial Wellness**
   - Financial literacy assessments
   - Risk tolerance questionnaires
   - Investment preference surveys

3. **Employee Surveys**
   - Compliance training assessments
   - Fraud awareness
   - Sales performance evaluations

4. **Market Research**
   - Product adoption surveys
   - Competitive analysis
   - Brand perception studies

**Best Practices:**
- Apply financial literacy-appropriate language
- Follow compliance standards (SEC, FINRA regulations)
- Use appropriate scales for risk assessment (often 7-point or 10-point)
- Structure questions for investment preferences
- Include regulatory-compliant disclaimers

**Common Scales:**
- Financial Capability Scale
- Financial Well-Being Scale (Consumer Financial Protection Bureau)
- Risk Aversion Scales

---

### Human Resources & Organizational

#### Common Survey Types:
1. **Employee Engagement**
   - Gallup Q12 framework
   - Utrecht Work Engagement Scale (UWES)
   - Job satisfaction indices

2. **360-Degree Feedback**
   - Multi-rater assessments
   - Leadership competency evaluations
   - Peer review systems

3. **Organizational Climate**
   - Culture assessments
   - Diversity & inclusion surveys
   - Psychological safety measures

4. **Performance Management**
   - Goal achievement surveys
   - Competency assessments
   - Development planning

**Best Practices:**
- Follow industrial-organizational psychology principles
- Ensure anonymity and psychological safety
- Use validated engagement instruments
- Structure for actionable insights
- Include both behavioral and attitudinal measures

**Validated Instruments:**
- Maslach Burnout Inventory
- Job Diagnostic Survey
- Organizational Commitment Questionnaire

---

### Marketing & Consumer Research

#### Common Survey Types:
1. **Customer Satisfaction (CSAT)**
   - Post-purchase satisfaction
   - Service quality (SERVQUAL)
   - Customer effort scores

2. **Market Research**
   - Concept testing
   - Price sensitivity (Van Westendorp)
   - Segmentation studies

3. **Brand Research**
   - Brand awareness and recall
   - Brand perception studies
   - Net Promoter Score (NPS)

4. **User Experience (UX)**
   - System Usability Scale (SUS)
   - Task completion surveys
   - Website satisfaction

**Best Practices:**
- Apply consumer behavior frameworks
- Use market research best practices
- Employ appropriate rating scales (Likert, semantic differential)
- Design for customer journey mapping
- Structure for actionable business insights

**Common Methodologies:**
- MaxDiff analysis for feature prioritization
- Conjoint analysis for product optimization
- Van Westendorp Price Sensitivity Meter

---

### Engineering & Technology

#### Common Survey Types:
1. **User Experience (UX) Research**
   - System Usability Scale (SUS)
   - Technology Acceptance Model (TAM)
   - User satisfaction questionnaires

2. **Technical Assessments**
   - Skills inventory surveys
   - Training needs analysis
   - Technical proficiency evaluations

3. **Product Development**
   - Feature prioritization
   - Beta testing feedback
   - Requirements gathering

4. **Safety & Compliance**
   - Safety culture assessments
   - Incident reporting surveys
   - Compliance training evaluations

**Best Practices:**
- Use industry-standard terminology
- Include quantitative metrics where appropriate
- Design for technical precision
- Structure questions for specifications
- Employ technical rating scales

---

### Legal & Compliance

#### Common Survey Types:
1. **Compliance Assessments**
   - Policy awareness surveys
   - Training effectiveness
   - Risk assessment questionnaires

2. **Ethics & Conduct**
   - Code of conduct surveys
   - Whistleblower climate
   - Ethical decision-making

3. **Legal Research**
   - Jury selection questionnaires
   - Witness credibility assessments
   - Legal needs surveys

**Best Practices:**
- Use precise legal terminology
- Ensure questions meet regulatory requirements
- Structure for documentation and evidence gathering
- Maintain professional standards for legal inquiry
- Include appropriate disclaimers

---

### Psychology & Social Sciences

#### Common Survey Types:
1. **Personality Assessments**
   - Big Five Inventory (BFI)
   - Myers-Briggs Type Indicator (MBTI)
   - NEO Personality Inventory

2. **Mental Health Screening**
   - PHQ-9 (depression)
   - GAD-7 (anxiety)
   - PTSD Checklist

3. **Social Research**
   - Attitude scales
   - Behavioral intention questionnaires
   - Social support measures

4. **Clinical Research**
   - Treatment outcome surveys
   - Symptom tracking
   - Quality of life assessments

**Best Practices:**
- Use validated psychological instruments
- Follow APA ethical guidelines
- Employ appropriate statistical methods
- Include informed consent
- Structure for diagnostic precision

**Key Resources:**
- APA PsycTests: Over 2 million psychological instruments
- Standards for Educational and Psychological Testing

---

## Validated Instruments & Scales

### Healthcare
- **PHQ-9**: 9-item Patient Health Questionnaire (Depression)
- **GAD-7**: 7-item Generalized Anxiety Disorder scale
- **SF-36**: 36-Item Short Form Health Survey
- **EQ-5D**: EuroQol 5-Dimension Quality of Life Scale
- **Copenhagen Burnout Inventory**: Healthcare professional burnout
- **Well-Being Index (WBI)**: 9-item clinician well-being

### Education
- **NSSE**: National Survey of Student Engagement
- **CLASSE**: Classroom Survey of Student Engagement
- **Teaching Perspectives Inventory**: Faculty teaching approaches

### Psychology
- **Big Five Inventory (BFI)**: Personality assessment
- **Beck Depression Inventory (BDI)**: Depression severity
- **State-Trait Anxiety Inventory (STAI)**: Anxiety assessment
- **Rosenberg Self-Esteem Scale**: Self-esteem measurement

### Organizational
- **Gallup Q12**: Employee engagement
- **Utrecht Work Engagement Scale (UWES)**: Work engagement
- **Maslach Burnout Inventory (MBI)**: Occupational burnout
- **Job Diagnostic Survey**: Job characteristics

### Marketing
- **Net Promoter Score (NPS)**: Customer loyalty
- **SERVQUAL**: Service quality assessment
- **System Usability Scale (SUS)**: Usability measurement
- **Customer Satisfaction Score (CSAT)**: Transaction satisfaction

---

## Question Design Principles

### 1. Clarity and Simplicity (Cognitive Load Reduction)

**Principles:**
- Use simple, clear language at 8th-grade reading level
- Avoid jargon unless domain-appropriate
- One idea per question (no double-barreled questions)
- Specific rather than abstract language

**Examples:**

❌ **Bad**: "How would you rate your satisfaction with the comprehensiveness and timeliness of the services provided?"
✅ **Good**:
- "How satisfied are you with the range of services provided?"
- "How satisfied are you with how quickly services were delivered?"

❌ **Bad**: "Do you agree that the utilization of our digital infrastructure has enhanced your operational efficiency?"
✅ **Good**: "Has using our online tools made your work easier?"

### 2. Question Types and When to Use Them

#### Closed-Ended Questions

**Likert Scales (Agreement)**
- When to use: Measuring attitudes, opinions, perceptions
- Standard: 5-point or 7-point
- Labels: Strongly Disagree → Strongly Agree

**Frequency Scales**
- When to use: Measuring behaviors, occurrences
- Standard: Never, Rarely, Sometimes, Often, Always
- Alternative: Specific timeframes (e.g., "In the past week...")

**Rating Scales**
- When to use: Evaluating quality, satisfaction, importance
- Common: 1-5 (Poor to Excellent), 1-10 (NPS)
- Healthcare: Often uses 0-10 pain scales

**Multiple Choice**
- When to use: Selecting from predefined options
- Best practice: Mutually exclusive, collectively exhaustive
- Include "Other (please specify)" when appropriate

**Ranking Questions**
- When to use: Prioritizing options
- Limitation: Cognitive load increases with options (max 5-7)
- Alternative: MaxDiff for large option sets

**Matrix/Grid Questions**
- When to use: Rating multiple items on same scale
- Best practice: Keep rows to 5-7 max
- Consideration: High cognitive load, use sparingly

#### Open-Ended Questions

**When to use:**
- Exploratory research
- Capturing unexpected insights
- Following up on closed-ended responses
- Qualitative data collection

**Best practices:**
- Provide clear instructions on expected response
- Indicate approximate length (e.g., "2-3 sentences")
- Use sparingly to avoid respondent fatigue

### 3. Question Order and Flow

**Funnel Sequence (General → Specific)**

```
1. Overall satisfaction with company
2. Satisfaction with specific departments
3. Satisfaction with individual interactions
4. Specific suggestions for improvement
```

**Benefits:**
- Establishes context
- Reduces response bias
- Creates natural progression

**Inverted Funnel (Specific → General)**

```
1. Rate your last customer service interaction
2. Rate customer service in past month
3. Overall opinion of company
```

**Benefits:**
- Useful for recall accuracy
- Grounds abstract concepts in concrete experiences

**Sensitive Question Placement:**
- **Demographic questions**: Place at end (reduce early termination)
- **Sensitive topics**: After rapport is built
- **Behavioral questions**: Before attitudinal questions (reduce consistency bias)

### 4. Avoiding Bias

**Leading Questions**
❌ **Bad**: "How much do you love our amazing product?"
✅ **Good**: "How satisfied are you with our product?"

**Loaded Questions**
❌ **Bad**: "Do you agree with experts that climate change is a serious threat?"
✅ **Good**: "How concerned are you about climate change?"

**Double-Barreled**
❌ **Bad**: "How satisfied are you with the speed and accuracy of our service?"
✅ **Good**: Two separate questions

**Acquiescence Bias**
- Mix positive and negative framing
- Use balanced scales with midpoints

**Social Desirability Bias**
- Use indirect questioning
- Emphasize anonymity
- Frame questions neutrally

### 5. Response Options Design

**Balanced Scales**
- Equal positive and negative options
- Include midpoint for neutral ("Neither agree nor disagree")
- Consistent labeling across survey

**Unbalanced Scales (when appropriate)**
- Satisfaction surveys often use 4-point (force direction)
- Performance evaluations may lean positive

**"Don't Know" / "Not Applicable" Options**
- Include when respondents may lack knowledge
- Don't force opinions on unfamiliar topics
- Reduces random responses

---

## Cognitive Load & User Experience

### Intrinsic Cognitive Load (ICL)
Effort required to process the information being sought

**Optimization strategies:**
- Use familiar language and concepts
- Provide clear definitions for technical terms
- Match question complexity to respondent expertise

### Extraneous Cognitive Load (ECL)
Mental effort caused by poor design, barriers to comprehension

**Reduction strategies:**

#### 1. Visual Design
- Clean, uncluttered layout
- Adequate white space
- Clear progress indicators
- Mobile-responsive design

#### 2. Navigation
- Logical question flow
- Clear skip patterns
- Back button availability
- Save-and-resume capability

#### 3. Instructions
- Brief, clear directions
- Examples when needed
- Format instructions near questions
- Use tooltips for additional context

#### 4. Question Density
- One question per screen for complex topics
- Group simple related questions
- Avoid matrix questions when possible

### Survey Length Optimization

**Time considerations:**
- **5-10 minutes**: Optimal for customer feedback
- **15-20 minutes**: Maximum for most surveys
- **20-30 minutes**: Only for highly motivated respondents (academic, employee)
- **30+ minutes**: Requires incentives, breaks

**Question count guidance:**
- Average: 2-3 questions per minute
- Matrix questions count as multiple
- Open-ended questions take 2-3x longer

### Mobile Optimization

**Best practices:**
- Vertical layout only
- Large touch targets (min 44x44 pixels)
- Minimize typing (use buttons, sliders)
- Avoid horizontal scrolling
- Test on multiple devices

---

## Academic Best Practices

### PhD-Level Survey Methodology

#### 1. Theoretical Framework
- Ground survey in established theory
- Clearly articulate hypotheses or research questions
- Link questions to theoretical constructs

#### 2. Literature Review
- Review existing validated instruments
- Identify gaps in current measures
- Build on established methodologies

#### 3. Pilot Testing

**Cognitive Interviewing:**
- Think-aloud protocols
- Probe for comprehension
- Identify ambiguities
- Typical n=5-15 participants

**Pilot Survey:**
- Test full survey with representative sample
- n=25-50 typical minimum
- Analyze completion time
- Check for data quality issues
- Refine based on results

#### 4. Construct Validity

**Convergent Validity:**
- Correlate with existing validated measures
- r > 0.50 typically desired

**Discriminant Validity:**
- Low correlation with theoretically distinct constructs
- r < 0.30 typically desired

**Known-Groups Validity:**
- Survey distinguishes between groups expected to differ
- Use t-tests or ANOVA to demonstrate

#### 5. Factor Analysis

**Exploratory Factor Analysis (EFA):**
- Identify underlying structure
- KMO > 0.70 required
- Eigenvalues > 1.0 for factor retention

**Confirmatory Factor Analysis (CFA):**
- Test theoretical structure
- Fit indices: CFI > 0.95, RMSEA < 0.06, SRMR < 0.08

#### 6. Reliability Assessment

**Internal Consistency:**
- Cronbach's α ≥ 0.70 (exploratory)
- Cronbach's α ≥ 0.80 (confirmatory)
- McDonald's ω for ordinal data

**Test-Retest:**
- 2-4 week interval typical
- ICC > 0.70 acceptable
- Pearson r > 0.70 minimum

#### 7. Sample Size Determination

**Rule of Thumb:**
- 10 respondents per item minimum
- 300+ for factor analysis
- 500+ for SEM (Structural Equation Modeling)

**Power Analysis:**
- Specify effect size
- Set α = 0.05, power = 0.80
- Calculate required n

#### 8. Response Rate Optimization

**Best practices:**
- Multiple contact attempts (3-5 optimal)
- Personalized invitations
- Advance notice
- Reminder emails (typically 3-7 days after)
- Incentives when appropriate

**Target response rates:**
- Online surveys: 20-30% typical
- Employee surveys: 60-70% expected
- Academic research: Varies by field

#### 9. Data Quality Checks

**Attention checks:**
- Instructed response items
- Consistency checks
- Time-to-completion flags

**Satisficing detection:**
- Straight-lining (same response repeatedly)
- Speeders (complete too quickly)
- Item non-response patterns

#### 10. Reporting Standards

**Include in methods:**
- Sample characteristics (demographics)
- Sampling method and recruitment
- Response rate calculation
- Survey instrument (full text in appendix)
- Reliability coefficients
- Validity evidence
- Data quality procedures

---

## Domain-Specific Question Banks

### Healthcare Example Questions

**Patient Satisfaction:**
- "How would you rate the quality of care you received?"
- "How well did the healthcare provider listen to your concerns?"
- "How clearly did the provider explain your treatment options?"
- "How satisfied were you with the coordination of your care?"

**Clinical Assessment (PHQ-9 style):**
- "Over the past 2 weeks, how often have you had little interest or pleasure in doing things?"
- Options: Not at all, Several days, More than half the days, Nearly every day

### Education Example Questions

**Course Evaluation:**
- "The course objectives were clearly communicated." [Likert]
- "The instructor provided helpful feedback on my work." [Likert]
- "How many hours per week did you typically spend on this course outside of class?" [Numeric]
- "What did you find most valuable about this course?" [Open-ended]

### Finance Example Questions

**Financial Literacy:**
- "Suppose you have $100 in a savings account earning 2% per year. After 5 years, how much would you have?" [Numeric]
- "If inflation is 3% and your savings earn 2%, is your purchasing power growing or shrinking?" [Multiple choice]

**Risk Tolerance:**
- "When making financial decisions, I tend to..." [Scale from Very Conservative to Very Aggressive]
- "How comfortable are you with investment values fluctuating?" [5-point scale]

### HR Example Questions

**Employee Engagement (Gallup Q12 style):**
- "I know what is expected of me at work." [Strongly Disagree to Strongly Agree]
- "In the last seven days, I have received recognition or praise for doing good work." [Yes/No or Frequency]
- "My supervisor, or someone at work, seems to care about me as a person." [Likert]

**360 Feedback:**
- "This person effectively communicates expectations." [5-point rating]
- "This person demonstrates leadership through their actions." [5-point rating]
- "What is this person's greatest strength?" [Open-ended]

---

## Implementation Guidelines

### For Survey Creators

**Step 1: Define Objectives**
- What decisions will this survey inform?
- What specific information is needed?
- Who is the target population?

**Step 2: Choose Survey Type**
- Exploratory vs. confirmatory
- Cross-sectional vs. longitudinal
- Descriptive vs. explanatory

**Step 3: Select or Develop Measures**
- Use validated instruments when available
- Adapt existing measures if necessary
- Develop new items following best practices

**Step 4: Design Question Flow**
- Opening: Engaging, easy questions
- Middle: Core content questions
- End: Demographics, sensitive topics

**Step 5: Pilot and Refine**
- Cognitive interviews (n=5-15)
- Pilot survey (n=25-50)
- Analyze and revise

**Step 6: Launch and Monitor**
- Track response rates
- Monitor completion patterns
- Check data quality in real-time

**Step 7: Analyze and Report**
- Clean data systematically
- Apply appropriate statistical methods
- Report following disciplinary standards

---

## Quality Indicators Checklist

### Survey Design Quality
- [ ] Clear research objectives defined
- [ ] Target population specified
- [ ] Appropriate sampling method
- [ ] Validated instruments used where available
- [ ] Questions pilot tested
- [ ] Cognitive load minimized
- [ ] Mobile-responsive design
- [ ] Clear instructions provided
- [ ] Progress indicators included
- [ ] Estimated completion time stated

### Methodological Rigor
- [ ] Theoretical framework established
- [ ] Literature review conducted
- [ ] Construct validity evidence
- [ ] Reliability coefficients calculated
- [ ] Sample size justified
- [ ] Data quality checks implemented
- [ ] Response rate optimization strategies
- [ ] Missing data plan established

### Reporting Standards
- [ ] Sample characteristics described
- [ ] Response rate calculated
- [ ] Survey instrument accessible
- [ ] Limitations acknowledged
- [ ] Validity evidence reported
- [ ] Statistical methods appropriate
- [ ] Results interpreted cautiously

---

## References and Resources

### Key Academic Papers
1. Taherdoost, H. (2022). Designing a Questionnaire for a Research Paper: A Comprehensive Guide
2. Krosnick, J. A. (2009). Question and Questionnaire Design (Stanford)
3. CROSS Guidelines (2024): Consensus-based Reporting of Survey Studies
4. Artino et al.: Survey Research Reporting Checklist
5. Standards for Educational and Psychological Testing (APA, AERA, NCME)

### Professional Organizations
- AAPOR (American Association for Public Opinion Research)
- CAHPS (Consumer Assessment of Healthcare Providers and Systems)
- IPEDS (Integrated Postsecondary Education Data System)

### Instrument Databases
- PsycTests (APA)
- CINAHL
- HaPI (Health and Psychosocial Instruments)
- PhenX Toolkit (NIH)

### Survey Platforms
- Qualtrics (enterprise, academic)
- SurveyMonkey (business, consumer)
- REDCap (research, healthcare)
- LimeSurvey (open-source)
- Typeform (UX-focused)

---

## Conclusion

This domain knowledge system provides a comprehensive framework for creating surveys across any professional domain, grounded in academic research and validated methodologies. By applying these principles, Surbee can generate surveys that meet the highest standards of survey methodology while being tailored to specific professional contexts and research goals.

The key is to balance methodological rigor with practical usability, always keeping the respondent experience and data quality at the forefront of design decisions.

---

**Document Version**: 1.0
**Last Updated**: 2025-01-21
**Maintained By**: Surbee Development Team
